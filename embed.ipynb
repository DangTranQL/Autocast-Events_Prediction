{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bf3fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import umap\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "# openai.api_key = os.getenv('OpenAI_API_Key')\n",
    "openai.api_key = \"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\"\n",
    "openai.Model.list()\n",
    "\n",
    "embeddings_location = \"C:/Users/ADMIN/Desktop/Few_shot-learning/code/embed_data.json\"\n",
    "# test = pd.read_csv(\"C:/Users/ADMIN/Desktop/Few_shot-learning/test_set.csv\")\n",
    "\n",
    "def make_embeddings(embeddings_location):\n",
    "    \"\"\"\n",
    "    Takes json files of questions using our json file formatting, \n",
    "        embeds them using OpenAI's embedding_engine,\n",
    "        and saves a new json, embeddings.json, of the embeddings.\n",
    "    \"\"\"\n",
    "    list_of_embeddings = []\n",
    "    ques = natsorted(glob(\"C:/Users/ADMIN/Desktop/Few_shot-learning/data/*\"))\n",
    "    \n",
    "    for q in ques:          \n",
    "        json_location = q\n",
    "        with open(json_location, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        raw_question = data['question']\n",
    "        embedding = openai.Embedding.create(input = raw_question, \n",
    "                                            model=\"text-embedding-ada-002\")['data'][0]['embedding']\n",
    "        list_of_embeddings.append(embedding)\n",
    "        time.sleep(1)\n",
    "\n",
    "    embeddings = {'list_of_embeddings':list_of_embeddings}\n",
    "    # print(embeddings)\n",
    "    with open(embeddings_location, 'w') as f:\n",
    "        f.write(json.dumps(embeddings))\n",
    "        \n",
    "def get_embeddings(embeddings_file):\n",
    "    \"\"\"\n",
    "    Retrieves embeddings from embeddings_file. Embeddings are assumed to be (n x d).\n",
    "    \"\"\"\n",
    "    with open(embeddings_file, 'r') as f:\n",
    "        points = json.load(f)['list_of_embeddings']\n",
    "    return np.array(points)\n",
    "\n",
    "def get_most_similar(embeddings, i):\n",
    "    \"\"\"\n",
    "    Returns most similar questions, while they are in their embedded form, \n",
    "        to the target, index i, via cosine similarity.\n",
    "    \"\"\"\n",
    "    cos_sims = []\n",
    "    cos_to_num = {}\n",
    "    for j in range(len(embeddings)):\n",
    "        cos_sim = util.cos_sim(embeddings[i-1], embeddings[j]).item()\n",
    "        cos_to_num[cos_sim] = j\n",
    "        cos_sims.append(cos_sim)\n",
    "    ordered = sorted(cos_sims, reverse=True)\n",
    "\n",
    "    closest_qs = []\n",
    "    for val in ordered:\n",
    "        closest_qs.append(cos_to_num[val]+1)\n",
    "    return closest_qs[1:]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    make_embeddings(embeddings_location)\n",
    "#     embeddings = get_embeddings(embeddings_location)\n",
    "#     for i in range(len(test[\"ID\"])):\n",
    "#         similar = get_most_similar(embeddings, i+1)\n",
    "#         print(test[\"ID\"][i])\n",
    "#         print(similar[0]-1)\n",
    "#         print(\"\\n\")\n",
    "    # reduced_points = reduce_via_umap(embeddings)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e4402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
